---
title: "How We Unified Entity Management in HomeForged’s Schema Pipeline"
date: "2025-11-20"
excerpt: "We refactored HomeForged's schema analysis system to unify entity handling—here's how pure analyzers and canonical structures changed the game."
tags: ["schema design", "data pipelines", "refactoring", "TypeScript", "entity management"]
author: "Ryan Dashwood"
---
## The Problem with Traversal Callbacks

When I first dug into HomeForged’s schema analysis pipeline, I was met with a tangle of traversal logic and scattered callbacks. The original system relied on an `AnalysisTraverser` class that walked the schema tree and fired off side-effect-heavy callbacks at each node. It worked—sort of—but came with hidden costs.

Validation logic was duplicated across multiple callback handlers. Normalization happened inconsistently, depending on who wrote the traversal step. And worst of all, extending the system meant touching fragile, interdependent code. Adding a new schema feature often broke unrelated parts of the output because entities weren’t treated as first-class, consistent structures.

We needed a shift from "do stuff as we walk" to "analyze, then act." That meant getting rid of traversal-side mutations and building a pipeline where entities were defined clearly, handled uniformly, and extended predictably.

## Phase 2: Moving to Pure Analyzers

The first real pivot came with replacing the `AnalysisTraverser` with functional, pure analyzers. Instead of letting traversal logic dictate behavior, we inverted control: the traversal became a dumb iterator, and analysis became a set of composable functions.

We introduced analyzers like `analyzeObjectEntity`, `analyzeArrayEntity`, and `analyzeScalarEntity`—each responsible for taking a schema node and returning a structured analysis result. These functions didn’t mutate anything. They didn’t write to shared state. They just *computed*.

```typescript
function analyzeObjectEntity(node: SchemaNode): ObjectAnalysis {
  return {
    type: 'object',
    properties: node.fields.map(analyzeField),
    required: node.requiredFields,
    metadata: extractMetadata(node)
  };
}
```

This shift alone made the system easier to test and debug. Each analyzer could be unit tested in isolation. We could simulate edge cases without spinning up the entire traversal context. And because the output was deterministic, we started catching inconsistencies in how different node types were interpreted.

But we weren’t done. While the analyzers were pure, the *entities* they produced still lacked a unified shape. An object entity looked different from an array entity, not just in structure but in how metadata was attached, how references were resolved, and how validation rules were applied. That inconsistency was a tax on every new feature.

## Phase 3: Enforcing Canonical Entity Structures

The real breakthrough came in Phase 3: entity unification. We defined a canonical interface for all schema entities, regardless of type:

```typescript
interface SchemaEntity {
  kind: EntityKind; // 'object', 'array', 'scalar', etc.
  id: string;
  path: string;
  metadata: Record<string, any>;
  validations: ValidationRule[];
  sourceNode: SchemaNode;
}
```

Every analyzer now returns a structure that conforms to this contract. This wasn’t just a typing exercise—it forced us to standardize how we handle identity, location, and rules across the board.

With canonical entities in place, we could build shared tooling on top. A single validation engine processes all entities. A unified normalizer ensures consistent defaults. And cross-cutting concerns like deprecation tracking or documentation generation became trivial to implement.

More importantly, the pipeline became *extensible*. When we added support for custom field directives, we didn’t need to modify traversal or rewrite validation logic. We just updated the entity schema and plugged in a new analyzer. The rest of the system just worked.

This unification also paved the way for better YAML output stability—something we’ll dive into in a future post—because now we’re generating from consistent, well-validated entities instead of raw, uneven schema nodes.

## What This Means Going Forward

Refactoring the schema pipeline wasn’t about chasing purity. It was about removing friction. Today, adding a new schema construct takes hours instead of days. Bugs are narrower and easier to trace. And the system behaves predictably, even under complex compositions.

If you’re working on a data pipeline or config system, ask: are your entities first-class citizens? Or are they just side effects of traversal? The answer might be costing you more than you think.
